{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from fastmri.data.mri_data import fetch_dir\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import UnetDataTransform\n",
    "from fastmri.pl_modules import FastMriDataModule, UnetModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "mask_type = \"equispaced\" # for brain\n",
    "center_fractions = [0.08, 0.04]\n",
    "accelerations = [4, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_mask_for_mask_type(mask_type, center_fractions, accelerations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"multicoil\" # CHALLENGE is either singlecoil or multicoil\n",
    "# resume_from_checkpoint = \"MODEL\" # where model is path to the model checkpoint\n",
    "\n",
    "# FastMriDataModule\n",
    "# Args:\n",
    "#     data_path: Path to root data directory. For example, if knee/path\n",
    "#         is the root directory with subdirectories multicoil_train and\n",
    "#         multicoil_val, you would input knee/path for data_path.\n",
    "#     challenge: Name of challenge from ('multicoil', 'singlecoil').\n",
    "#     train_transform: A transform object for the training split.\n",
    "#     val_transform: A transform object for the validation split.\n",
    "#     test_transform: A transform object for the test split.\n",
    "#     combine_train_val: Whether to combine train and val splits into one\n",
    "#         large train dataset. Use this for leaderboard submission.\n",
    "#     test_split: Name of test split from (\"test\", \"challenge\").\n",
    "#     test_path: An optional test path. Passing this overwrites data_path\n",
    "#         and test_split.\n",
    "#     sample_rate [optional]: Fraction of slices of the training data split to use.\n",
    "#         Can be set to less than 1.0 for rapid prototyping. If not set,\n",
    "#         it defaults to 1.0. To subsample the dataset either set\n",
    "#         sample_rate (sample by slice) or volume_sample_rate (sample by\n",
    "#         volume), but not both.\n",
    "#     val_sample_rate: Same as sample_rate, but for val split.\n",
    "#     test_sample_rate: Same as sample_rate, but for test split.\n",
    "#     volume_sample_rate: Fraction of volumes of the training data split\n",
    "#         to use. Can be set to less than 1.0 for rapid prototyping. If\n",
    "#         not set, it defaults to 1.0. To subsample the dataset either\n",
    "#         set sample_rate (sample by slice) or volume_sample_rate (sample\n",
    "#         by volume), but not both.\n",
    "#     val_volume_sample_rate: Same as volume_sample_rate, but for val\n",
    "#         split.\n",
    "#     test_volume_sample_rate: Same as volume_sample_rate, but for val\n",
    "#         split.\n",
    "#     train_filter: A callable which takes as input a training example\n",
    "#         metadata, and returns whether it should be part of the training\n",
    "#         dataset.\n",
    "#     val_filter: Same as train_filter, but for val split.\n",
    "#     test_filter: Same as train_filter, but for test split.\n",
    "#     use_dataset_cache_file: Whether to cache dataset metadata. This is\n",
    "#         very useful for large datasets like the brain data.\n",
    "#     batch_size: Batch size.\n",
    "#     num_workers: Number of workers for PyTorch dataloader.\n",
    "#     distributed_sampler: Whether to use a distributed sampler. This\n",
    "#         should be set to True if training with ddp.\n",
    "\n",
    "data_path=\"\" # train data path\n",
    "test_split=\"\" # TESTSPLIT should specify the test split you want to run on - either test or challenge.\n",
    "test_path=\"\" # test data path\n",
    "sample_rate=\"1.0\" # define\n",
    "batch_size=\"1\" #  batch_size = 1 if backend == \"ddp\" else num_gpus\n",
    "num_workers=\"4\" # default 4\n",
    "distributed_sampler=\"True\" # Set to True if training with ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = UnetDataTransform(challenge, mask, use_seed=False)\n",
    "val_transform = UnetDataTransform(challenge, mask)\n",
    "test_transform = UnetDataTransform(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = FastMriDataModule(\n",
    "    data_path=data_path,\n",
    "    challenge=challenge,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=test_transform,  \n",
    "    combine_train_val=True,\n",
    "    test_split=test_split,\n",
    "    test_path=test_path,\n",
    "    sample_rate=sample_rate,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    distributed_sampler=distributed_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UnetModule(\n",
    "#     in_chans=1,\n",
    "#     out_chans=,\n",
    "#     chans=,\n",
    "#     num_pool_layers=,\n",
    "#     drop_prob=,\n",
    "#     lr=,\n",
    "#     lr_step_size=,\n",
    "#     lr_gamma=,\n",
    "#     weight_decay=,\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args:\n",
    "    # gpus: The number of GPUs to use for training. For example, num_gpus is used to set this in the code.\n",
    "    # strategy: The distributed training strategy, such as \"ddp\" for Distributed Data Parallel.\n",
    "    # seed: The random seed for reproducibility.\n",
    "    # deterministic: If set to True, this makes the training deterministic (but potentially slower).\n",
    "    # default_root_dir: The default root directory for saving model checkpoints and logs.\n",
    "    # max_epochs: The maximum number of epochs for training.\n",
    "    # callbacks: A list of callbacks for various training events, such as model checkpointing.\n",
    "\n",
    "gpus = 1\n",
    "strategy = \"ddp_notebook\"\n",
    "seed = 42\n",
    "deterministic = True\n",
    "default_root_dir = \"\"\n",
    "max_epoch = 100 \n",
    "\n",
    "\n",
    "trainer = pl.Trainer(num_nodes=gpus, strategy=strategy, deterministic=deterministic, default_root_dir=default_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastmri-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
